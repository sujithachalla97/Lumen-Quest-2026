{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Telco Customer Churn - Feature Engineering\n",
    "\n",
    "This notebook performs comprehensive feature engineering for the Telco Customer Churn dataset.\n",
    "\n",
    "**Objectives**:\n",
    "- Clean and preprocess data\n",
    "- Create new meaningful features\n",
    "- Encode categorical variables\n",
    "- Scale numerical features\n",
    "- Prepare data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"üõ†Ô∏è Feature Engineering Libraries Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/Telco-Customer-Churn.csv')\n",
    "    print(f\"‚úÖ Dataset loaded: {df.shape}\")\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Please run 01_data_exploration.ipynb first\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nüìä Dataset overview:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "print(\"üßπ DATA CLEANING\")\n",
    "\n",
    "# Check for data types and potential issues\n",
    "print(\"\\nüìä Current data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any columns that should be numeric but are object type\n",
    "print(\"\\nüîç Checking for numeric columns stored as objects:\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' and col not in ['customerID', 'Churn']:\n",
    "        # Try to convert to numeric\n",
    "        try:\n",
    "            numeric_conversion = pd.to_numeric(df[col], errors='coerce')\n",
    "            if numeric_conversion.notna().sum() > 0:\n",
    "                print(f\"  {col}: Can be converted to numeric\")\n",
    "                # Check for spaces or special characters\n",
    "                unique_values = df[col].unique()\n",
    "                print(f\"    Unique values: {unique_values[:5]}\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle TotalCharges column (commonly has issues in this dataset)\n",
    "if 'TotalCharges' in df.columns:\n",
    "    print(\"üîß Fixing TotalCharges column:\")\n",
    "    \n",
    "    # Check current state\n",
    "    print(f\"Current dtype: {df['TotalCharges'].dtype}\")\n",
    "    print(f\"Sample values: {df['TotalCharges'].head().tolist()}\")\n",
    "    \n",
    "    # Convert to numeric, handling spaces and errors\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df['TotalCharges'].isna().sum()\n",
    "    print(f\"Missing values after conversion: {missing_count}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        # Fill missing values with median or 0 for new customers\n",
    "        # If tenure is 0, set TotalCharges to 0\n",
    "        if 'tenure' in df.columns:\n",
    "            df.loc[(df['tenure'] == 0) & (df['TotalCharges'].isna()), 'TotalCharges'] = 0\n",
    "            # Fill remaining missing values with median\n",
    "            df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
    "        else:\n",
    "            df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Missing values handled\")\n",
    "    \n",
    "    print(f\"Final dtype: {df['TotalCharges'].dtype}\")\n",
    "    print(f\"Final stats: {df['TotalCharges'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Target Variable Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable\n",
    "print(\"üéØ TARGET VARIABLE PREPARATION\")\n",
    "\n",
    "if 'Churn' in df.columns:\n",
    "    # Convert target to binary\n",
    "    df['Churn_Binary'] = (df['Churn'] == 'Yes').astype(int)\n",
    "    \n",
    "    print(f\"Target distribution:\")\n",
    "    print(df['Churn'].value_counts())\n",
    "    print(f\"\\nBinary target distribution:\")\n",
    "    print(df['Churn_Binary'].value_counts())\n",
    "    print(f\"\\nChurn rate: {df['Churn_Binary'].mean():.2%}\")\nelse:\n",
    "    print(\"‚ùå Churn column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "print(\"üÜï CREATING NEW FEATURES\")\n",
    "\n",
    "# 1. Monthly charges per service ratio\n",
    "if 'MonthlyCharges' in df.columns and 'tenure' in df.columns:\n",
    "    # Average monthly charges (handling division by zero)\n",
    "    df['AvgMonthlyCharges'] = df['MonthlyCharges']\n",
    "    \n",
    "    # Total charges per tenure month\n",
    "    if 'TotalCharges' in df.columns:\n",
    "        df['ChargesPerTenure'] = df['TotalCharges'] / (df['tenure'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    print(\"‚úÖ Charges-based features created\")\n",
    "\n",
    "# 2. Service count features\n",
    "service_cols = []\n",
    "for col in df.columns:\n",
    "    if any(service in col.lower() for service in ['phone', 'internet', 'online', 'backup', 'protection', 'support', 'streaming']):\n",
    "        if df[col].dtype == 'object' and 'Yes' in df[col].unique():\n",
    "            service_cols.append(col)\n",
    "\n",
    "if service_cols:\n",
    "    print(f\"\\nService columns identified: {service_cols}\")\n",
    "    \n",
    "    # Count total services\n",
    "    df['TotalServices'] = 0\n",
    "    for col in service_cols:\n",
    "        df['TotalServices'] += (df[col] == 'Yes').astype(int)\n",
    "    \n",
    "    # Service penetration rate\n",
    "    df['ServicePenetration'] = df['TotalServices'] / len(service_cols)\n",
    "    \n",
    "    print(f\"‚úÖ Service count features created\")\n",
    "    print(f\"Total services stats: {df['TotalServices'].describe()}\")\n",
    "\n",
    "# 3. Customer lifetime value features\n",
    "if 'MonthlyCharges' in df.columns and 'tenure' in df.columns:\n",
    "    # Customer lifetime value estimation\n",
    "    df['EstimatedCLV'] = df['MonthlyCharges'] * df['tenure']\n",
    "    \n",
    "    # Monthly charges to tenure ratio\n",
    "    df['ChargesTenureRatio'] = df['MonthlyCharges'] / (df['tenure'] + 1)\n",
    "    \n",
    "    print(\"‚úÖ Customer value features created\")\n",
    "\n",
    "# 4. Contract and payment features\n",
    "if 'Contract' in df.columns:\n",
    "    # Contract length mapping\n",
    "    contract_mapping = {\n",
    "        'Month-to-month': 1,\n",
    "        'One year': 12,\n",
    "        'Two year': 24\n",
    "    }\n",
    "    df['ContractLength'] = df['Contract'].map(contract_mapping)\n",
    "    \n",
    "    # Long-term contract indicator\n",
    "    df['LongTermContract'] = (df['ContractLength'] >= 12).astype(int)\n",
    "    \n",
    "    print(\"‚úÖ Contract features created\")\n",
    "\n",
    "# 5. Demographic features\n",
    "if 'SeniorCitizen' in df.columns and 'tenure' in df.columns:\n",
    "    # Senior citizen with long tenure\n",
    "    df['SeniorLongTenure'] = ((df['SeniorCitizen'] == 1) & (df['tenure'] > 12)).astype(int)\n",
    "    \n",
    "    print(\"‚úÖ Demographic features created\")\n",
    "\n",
    "print(f\"\\nüìä Total features after creation: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable encoding\n",
    "print(\"üè∑Ô∏è CATEGORICAL ENCODING\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target and ID columns\n",
    "exclude_cols = ['customerID', 'Churn']\n",
    "categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"Categorical columns to encode: {categorical_cols}\")\n",
    "\n",
    "# Create a copy for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Binary encoding for Yes/No columns\n",
    "binary_cols = []\n",
    "for col in categorical_cols:\n",
    "    unique_values = df[col].unique()\n",
    "    if len(unique_values) == 2 and set(unique_values) <= {'Yes', 'No'}:\n",
    "        binary_cols.append(col)\n",
    "        df_encoded[col] = (df[col] == 'Yes').astype(int)\n",
    "\n",
    "print(f\"Binary encoded columns: {binary_cols}\")\n",
    "\n",
    "# One-hot encoding for multi-category columns\n",
    "remaining_categorical = [col for col in categorical_cols if col not in binary_cols]\n",
    "\n",
    "if remaining_categorical:\n",
    "    print(f\"One-hot encoding columns: {remaining_categorical}\")\n",
    "    \n",
    "    for col in remaining_categorical:\n",
    "        # Create dummy variables\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "        # Drop original column\n",
    "        df_encoded.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    print(\"‚úÖ One-hot encoding completed\")\n",
    "\n",
    "print(f\"\\nüìä Features after encoding: {df_encoded.shape[1]}\")\n",
    "print(f\"New columns: {[col for col in df_encoded.columns if col not in df.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"üìè FEATURE SCALING\")\n",
    "\n",
    "# Identify numerical columns for scaling\n",
    "exclude_from_scaling = ['customerID', 'Churn', 'Churn_Binary'] + \\\n",
    "                      [col for col in df_encoded.columns if col.endswith('_Yes') or col.endswith('_No')] + \\\n",
    "                      binary_cols\n",
    "\n",
    "numerical_cols = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cols_to_scale = [col for col in numerical_cols if col not in exclude_from_scaling]\n",
    "\n",
    "print(f\"Columns to scale: {cols_to_scale}\")\n",
    "\n",
    "if cols_to_scale:\n",
    "    # Create scaled version\n",
    "    df_scaled = df_encoded.copy()\n",
    "    \n",
    "    # StandardScaler for features with normal distribution\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[cols_to_scale] = scaler.fit_transform(df_encoded[cols_to_scale])\n",
    "    \n",
    "    print(\"‚úÖ Standard scaling completed\")\n",
    "    \n",
    "    # Show scaling effect\n",
    "    print(\"\\nüìä Scaling effect:\")\n",
    "    scaling_comparison = pd.DataFrame({\n",
    "        'Original_Mean': df_encoded[cols_to_scale].mean(),\n",
    "        'Original_Std': df_encoded[cols_to_scale].std(),\n",
    "        'Scaled_Mean': df_scaled[cols_to_scale].mean(),\n",
    "        'Scaled_Std': df_scaled[cols_to_scale].std()\n",
    "    })\n",
    "    print(scaling_comparison.head())\nelse:\n",
    "    df_scaled = df_encoded.copy()\n",
    "    print(\"No columns need scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature selection and preparation\n",
    "print(\"üéØ FINAL FEATURE SELECTION\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['customerID', 'Churn']  # Keep Churn_Binary as target\n",
    "feature_columns = [col for col in df_scaled.columns if col not in columns_to_remove]\n",
    "\n",
    "# Prepare final dataset\n",
    "df_final = df_scaled[feature_columns].copy()\n",
    "\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"Features: {len(feature_columns) - 1}\")  # -1 for target column\n",
    "print(f\"Target column: Churn_Binary\")\n",
    "\n",
    "# Show final feature list\n",
    "features = [col for col in feature_columns if col != 'Churn_Binary']\n",
    "print(f\"\\nüìã Final features ({len(features)}):\")\nfor i, feature in enumerate(features, 1):\n    print(f\"{i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with target\n",
    "print(\"üìä FEATURE CORRELATION WITH TARGET\")\n",
    "\n",
    "if 'Churn_Binary' in df_final.columns:\n",
    "    # Calculate correlation with target\n",
    "    target_correlation = df_final.corr()['Churn_Binary'].drop('Churn_Binary')\n",
    "    target_correlation = target_correlation.sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Top 15 features correlated with churn:\")\n",
    "    print(target_correlation.head(15))\n",
    "    \n",
    "    # Visualize top correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_correlations = target_correlation.head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_correlations.values]\n",
    "    \n",
    "    plt.barh(range(len(top_correlations)), top_correlations.values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_correlations)), top_correlations.index)\n",
    "    plt.xlabel('Correlation with Churn')\n",
    "    plt.title('Top 15 Features Correlation with Churn')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature correlation matrix (top features)\n",
    "    top_features = target_correlation.head(10).index.tolist() + ['Churn_Binary']\n",
    "    correlation_matrix = df_final[top_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f')\n",
    "    plt.title('Correlation Matrix - Top Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features to reduce multicollinearity\n",
    "print(\"‚úÇÔ∏è REMOVING HIGHLY CORRELATED FEATURES\")\n",
    "\n",
    "# Calculate correlation matrix (excluding target)\n",
    "feature_cols = [col for col in df_final.columns if col != 'Churn_Binary']\n",
    "feature_corr_matrix = df_final[feature_cols].corr()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "threshold = 0.8  # Correlation threshold\n",
    "\n",
    "for i in range(len(feature_corr_matrix.columns)):\n",
    "    for j in range(i+1, len(feature_corr_matrix.columns)):\n",
    "        if abs(feature_corr_matrix.iloc[i, j]) > threshold:\n",
    "            col1 = feature_corr_matrix.columns[i]\n",
    "            col2 = feature_corr_matrix.columns[j]\n",
    "            corr_value = feature_corr_matrix.iloc[i, j]\n",
    "            high_corr_pairs.append((col1, col2, corr_value))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"Found {len(high_corr_pairs)} highly correlated pairs (threshold: {threshold}):\")\n",
    "    for col1, col2, corr in high_corr_pairs:\n",
    "        print(f\"  {col1} <-> {col2}: {corr:.3f}\")\n",
    "    \n",
    "    # Remove features with lower correlation to target\n",
    "    features_to_remove = set()\n",
    "    for col1, col2, corr in high_corr_pairs:\n",
    "        # Keep the feature with higher absolute correlation to target\n",
    "        corr1 = abs(target_correlation.get(col1, 0))\n",
    "        corr2 = abs(target_correlation.get(col2, 0))\n",
    "        \n",
    "        if corr1 < corr2:\n",
    "            features_to_remove.add(col1)\n",
    "        else:\n",
    "            features_to_remove.add(col2)\n",
    "    \n",
    "    print(f\"\\nRemoving {len(features_to_remove)} features: {list(features_to_remove)}\")\n",
    "    \n",
    "    # Create final dataset without highly correlated features\n",
    "    final_features = [col for col in df_final.columns if col not in features_to_remove]\n",
    "    df_final = df_final[final_features]\n",
    "    \n",
    "    print(f\"Final dataset shape after correlation removal: {df_final.shape}\")\nelse:\n",
    "    print(f\"No highly correlated features found (threshold: {threshold})\")\n",
    "\nprint(f\"\\nüìä Final dataset ready for modeling: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final processed dataset\n",
    "print(\"üíæ SAVING PROCESSED DATA\")\n",
    "\n",
    "# Save full processed dataset\n",
    "df_final.to_csv('../data/telco_processed_features.csv', index=False)\n",
    "print(\"‚úÖ Full processed dataset saved as '../data/telco_processed_features.csv'\")\n",
    "\n",
    "# Save feature names for future use\n",
    "feature_names = [col for col in df_final.columns if col != 'Churn_Binary']\n",
    "pd.DataFrame({'feature_names': feature_names}).to_csv('../data/feature_names.csv', index=False)\n",
    "print(\"‚úÖ Feature names saved as '../data/feature_names.csv'\")\n",
    "\n",
    "# Save preprocessing summary\n",
    "preprocessing_summary = {\n",
    "    'original_shape': df.shape,\n",
    "    'final_shape': df_final.shape,\n",
    "    'features_created': df_final.shape[1] - df.shape[1],\n",
    "    'churn_rate': df_final['Churn_Binary'].mean(),\n",
    "    'total_features': len(feature_names),\n",
    "    'categorical_encoded': len(binary_cols) + len(remaining_categorical),\n",
    "    'features_scaled': len(cols_to_scale) if 'cols_to_scale' in locals() else 0\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([preprocessing_summary])\n",
    "summary_df.to_csv('../data/preprocessing_summary.csv', index=False)\n",
    "print(\"‚úÖ Preprocessing summary saved as '../data/preprocessing_summary.csv'\")\n",
    "\n",
    "print(\"\\nüìã FEATURE ENGINEERING SUMMARY:\")\n",
    "print(f\"‚úÖ Original dataset: {df.shape}\")\n",
    "print(f\"‚úÖ Final dataset: {df_final.shape}\")\n",
    "print(f\"‚úÖ Features for modeling: {len(feature_names)}\")\n",
    "print(f\"‚úÖ Churn rate: {df_final['Churn_Binary'].mean():.2%}\")\n",
    "print(f\"‚úÖ Data ready for machine learning!\")\n",
    "\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"1. Run notebook: 03_exploratory_data_analysis.ipynb\")\n",
    "print(\"2. Run notebook: 04_machine_learning_models.ipynb\")\n",
    "print(\"3. Run notebook: 05_model_evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}